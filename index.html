<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Camera Interaction App</title>
  <style>
    body {
      font-family: sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 20px;
      padding: 20px;
      background-color: #f0f0f0;
    }
    video {
      width: 100%;
      max-width: 480px;
      border-radius: 8px;
      border: 2px solid #333;
      background-color: #000;
    }
    textarea {
      width: 100%;
      max-width: 480px;
      height: 60px;
      padding: 8px;
      border: 1px solid #ccc;
      border-radius: 4px;
      font-size: 14px;
    }
    button {
      padding: 12px;
      font-size: 16px;
      border-radius: 4px;
      border: none;
      color: white;
      cursor: pointer;
    }
    .start { background-color: #28a745; }
    .stop { background-color: #dc3545; }
    .switch { background-color: #007bff; }
  </style>
</head>
<body>
  <h1>Camera Interaction App</h1>

  <video id="videoFeed" autoplay playsinline muted></video>

  <textarea id="instructionText">What do you see?</textarea>
  <textarea id="responseText" readonly placeholder="Server response..."></textarea>

  <div>
    <button id="startButton" class="start">Start</button>
    <button id="switchCamera" class="switch">Switch Camera</button>
  </div>

  <script type="module">
    import { AutoProcessor, AutoModelForVision2Seq, RawImage } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers/dist/transformers.min.js";

    const video = document.getElementById('videoFeed');
    const instructionText = document.getElementById('instructionText');
    const responseText = document.getElementById('responseText');
    const startButton = document.getElementById('startButton');
    const switchCameraBtn = document.getElementById('switchCamera');

    let stream;
    let facingMode = 'environment';
    let isProcessing = false;
    let processor, model;

    async function initCamera() {
      if (stream) stream.getTracks().forEach(track => track.stop());
      const constraints = { video: { facingMode }, audio: false };
      try {
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        responseText.value = `Camera ready (${facingMode}).`;
      } catch (err) {
        responseText.value = `Camera error: ${err.message}`;
      }
    }

    async function initModel() {
      responseText.value = "Loading model...";
      processor = await AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM-500M-Instruct");
      model = await AutoModelForVision2Seq.from_pretrained("HuggingFaceTB/SmolVLM-500M-Instruct", {
        dtype: { embed_tokens: "fp16", vision_encoder: "q4", decoder_model_merged: "q4" },
        device: "webgpu",
      });
      responseText.value = "Model loaded.";
    }

    function captureImage() {
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(video, 0, 0);
      const frame = ctx.getImageData(0, 0, canvas.width, canvas.height);
      return new RawImage(frame.data, frame.width, frame.height, 4);
    }

    async function runInference() {
      if (!isProcessing) return;
      const image = captureImage();
      const instruction = instructionText.value;
      const messages = [{ role: "user", content: [{ type: "image" }, { type: "text", text: instruction }] }];
      const inputs = await processor(processor.apply_chat_template(messages, { add_generation_prompt: true }), [image]);
      const output = await model.generate({ ...inputs, max_new_tokens: 100 });
      responseText.value = processor.batch_decode(output.slice(null, [inputs.input_ids.dims.at(-1), null]), { skip_special_tokens: true })[0].trim();
      if (isProcessing) requestAnimationFrame(runInference);
    }

    startButton.onclick = () => {
      isProcessing = !isProcessing;
      startButton.textContent = isProcessing ? 'Stop' : 'Start';
      startButton.classList.toggle('start');
      startButton.classList.toggle('stop');
      if (isProcessing) runInference();
    };

    switchCameraBtn.onclick = () => {
      facingMode = facingMode === 'environment' ? 'user' : 'environment';
      initCamera();
    };

    window.onload = async () => {
      await initModel();
      await initCamera();
    };
  </script>
</body>
</html>
