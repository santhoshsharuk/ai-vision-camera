<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Camera Interaction App</title>
  <style>
    body {
      font-family: sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 20px;
      padding: 20px;
      background-color: #f0f0f0;
    }
    video {
      width: 100%;
      max-width: 480px;
      border-radius: 8px;
      border: 2px solid #333;
      background-color: #000;
    }
    .io-areas {
      display: flex;
      flex-direction: column;
      gap: 10px;
      width: 100%;
      max-width: 480px;
    }
    .io-areas label {
      font-weight: bold;
      margin-bottom: 4px;
    }
    .io-areas textarea {
      width: 100%;
      height: 60px;
      padding: 8px;
      border: 1px solid #ccc;
      border-radius: 4px;
      font-size: 14px;
      resize: vertical;
    }
    button {
      padding: 12px;
      font-size: 16px;
      border-radius: 4px;
      border: none;
      color: white;
      cursor: pointer;
      margin: 0 5px;
    }
    .start { background-color: #28a745; }
    .stop { background-color: #dc3545; }
    .switch { background-color: #007bff; }
  </style>
</head>
<body>
  <h1>Camera Interaction App</h1>

  <video id="videoFeed" autoplay playsinline muted></video>

  <div class="io-areas">
    <div>
      <label for="instructionText">Instruction:</label>
      <textarea id="instructionText">What do you see?</textarea>
    </div>
    <div>
      <label for="responseText">Response:</label>
      <textarea id="responseText" readonly placeholder="Server response will appear here..."></textarea>
    </div>
  </div>

  <div>
    <button id="startButton" class="start">Start</button>
    <button id="switchCamera" class="switch">Switch Camera</button>
  </div>

  <script type="module">
    import { AutoProcessor, AutoModelForVision2Seq, RawImage } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers/dist/transformers.min.js";

    const video = document.getElementById('videoFeed');
    const instructionText = document.getElementById('instructionText');
    const responseText = document.getElementById('responseText');
    const startButton = document.getElementById('startButton');
    const switchCameraBtn = document.getElementById('switchCamera');

    let stream;
    let facingMode = 'environment';
    let isProcessing = false;
    let processor, model;
    let deviceType;

    async function initCamera() {
      if (stream) stream.getTracks().forEach(track => track.stop());
      try {
        const constraints = { video: { facingMode }, audio: false };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        responseText.value = `Camera ready (${facingMode}).`;
      } catch (err) {
        responseText.value = `Camera error: ${err.message}`;
      }
    }

    async function initModel() {
      responseText.value = "Initializing model...";
      const modelId = "HuggingFaceTB/SmolVLM-500M-Instruct";
      deviceType = navigator.gpu ? 'webgpu' : 'cpu';

      // Load processor
      try {
        processor = await AutoProcessor.from_pretrained(modelId);
      } catch (err) {
        responseText.value = "Processor load error: " + err.message;
        return;
      }

      // Load model with fallback
      const loadOptions = { device: deviceType };
      if (deviceType === 'webgpu') {
        loadOptions.dtype = { embed_tokens: 'fp16', vision_encoder: 'q4', decoder_model_merged: 'q4' };
      }

      try {
        model = await AutoModelForVision2Seq.from_pretrained(modelId, loadOptions);
        responseText.value = `Model loaded on ${deviceType}.`;
      } catch (err) {
        responseText.value = `Model load error (${deviceType}): ${err.message}`;
        return;
      }

      // Start camera after model ready
      initCamera();
    }

    function captureImage() {
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(video, 0, 0);
      const frame = ctx.getImageData(0, 0, canvas.width, canvas.height);
      return new RawImage(frame.data, frame.width, frame.height, 4);
    }

    async function runInference() {
      if (!isProcessing) return;
      const img = captureImage();
      const instruction = instructionText.value;
      responseText.value = 'Processing...';
      try {
        const messages = [{ role: 'user', content: [{ type: 'image' }, { type: 'text', text: instruction }] }];
        const prompt = processor.apply_chat_template(messages, { add_generation_prompt: true });
        const inputs = await processor(prompt, [img], { do_image_splitting: false });
        const gen = await model.generate({ ...inputs, max_new_tokens: 100 });
        const out = processor.batch_decode(gen.slice(null, [inputs.input_ids.dims.at(-1), null]), { skip_special_tokens: true });
        responseText.value = out[0].trim();
      } catch (err) {
        responseText.value = 'Inference error: ' + err.message;
      }
      if (isProcessing) requestAnimationFrame(runInference);
    }

    startButton.onclick = () => {
      isProcessing = !isProcessing;
      startButton.textContent = isProcessing ? 'Stop' : 'Start';
      startButton.classList.toggle('start');
      startButton.classList.toggle('stop');
      if (isProcessing) runInference();
    };

    switchCameraBtn.onclick = () => {
      facingMode = facingMode === 'environment' ? 'user' : 'environment';
      initCamera();
    };

    window.addEventListener('load', initModel);
  </script>
</body>
</html>
